# Model architecture configuration for SMART Planner training.
# Corresponds to the "balanced" ~150M parameter decoder-only transformer.

model:
  name: "smart-planner-150m"
  architecture: "decoder-only-transformer"

  # Core dimensions
  d_model: 768
  n_layers: 12
  n_heads: 12
  n_kv_heads: 4         # Grouped-query attention (GQA)
  d_ff: 2048             # SwiGLU intermediate dimension

  # Vocabulary and sequence
  vocab_size: 32000      # BPE/SentencePiece
  max_seq_length: 1024

  # Architecture choices
  position_embedding: "rope"
  rope_theta: 10000.0
  norm_type: "rmsnorm"
  norm_eps: 1.0e-5
  activation: "swiglu"
  dropout: 0.0           # Small models may benefit from 0.05-0.1

  # Weight initialisation
  init_std: 0.02

# Tokenizer
tokenizer:
  type: "sentencepiece"
  vocab_size: 32000
  model_type: "bpe"
  pad_token: "<|pad|>"
  bos_token: "<|begin|>"
  eos_token: "<|end|>"
  special_tokens:
    - "<|json|>"
    - "<|/json|>"
    - "<|system|>"
    - "<|user|>"
    - "<|assistant|>"

# Smaller variant for testing
variants:
  ultra_tiny:
    name: "smart-planner-35m"
    d_model: 512
    n_layers: 8
    n_heads: 8
    n_kv_heads: 4
    d_ff: 1408
    vocab_size: 16384

  higher_quality:
    name: "smart-planner-350m"
    d_model: 1024
    n_layers: 24
    n_heads: 16
    n_kv_heads: 4
    d_ff: 2816
    vocab_size: 32000
