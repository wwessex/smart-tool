# Direct Preference Optimisation (DPO) configuration for Stage 3.
#
# Goal: improve output quality by training on pairwise preference data
# (chosen vs rejected). Makes outputs more realistic, less verbose,
# less biased, and better aligned with SMART criteria.

training:
  stage: "dpo"

  # Data
  data:
    # Pairwise preference comparisons scored by rubric
    sources:
      - name: "quality_preferences"
        weight: 0.5
        description: "Better vs worse SMART action quality"
        pair_count: 30000
      - name: "safety_preferences"
        weight: 0.3
        description: "Safe vs biased/harmful outputs"
        pair_count: 15000
      - name: "conciseness_preferences"
        weight: 0.2
        description: "Concise vs verbose outputs"
        pair_count: 10000
    total_pairs: 55000

  # DPO hyperparameters
  dpo:
    beta: 0.1                # KL penalty coefficient
    label_smoothing: 0.0     # No label smoothing initially
    loss_type: "sigmoid"     # Standard DPO loss

  # Optimiser
  optimizer:
    type: "adamw"
    lr: 5.0e-5              # Lower than SFT
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1.0e-8

  # Schedule
  schedule:
    warmup_steps: 200
    total_steps: 5000
    decay: "cosine"
    min_lr: 5.0e-6

  # Batch
  batch:
    micro_batch_size: 4
    gradient_accumulation_steps: 4
    # Each example is a pair (chosen + rejected), so effective size is 2x

  # Precision
  precision: "bf16"
  gradient_clipping: 1.0

  # Reference model
  reference_model:
    # The SFT checkpoint serves as the reference model for KL divergence
    path: "./checkpoints/sft/latest"
    frozen: true

  # Checkpointing
  checkpoint:
    save_every_steps: 500
    keep_last_n: 3
    output_dir: "./checkpoints/dpo"
    load_from: "./checkpoints/sft/latest"

  # Evaluation
  eval:
    eval_every_steps: 250
    num_eval_pairs: 100
    metrics:
      - "preference_accuracy"  # % where model prefers chosen over rejected
      - "smart_score"          # Average SMART criteria score on eval set
      - "json_parse_rate"      # Ensure DPO doesn't degrade format compliance
